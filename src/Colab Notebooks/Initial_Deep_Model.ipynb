{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Initial Deep Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZWYLY4sFkcQ"
      },
      "source": [
        "For a smooth run, Save the file I shared on google in the following location on your drive:\n",
        "\n",
        "/content/drive/MyDrive/DataFiles/\n",
        "\n",
        "You will have to make a DataFiles folder in your drive home page and then add the shared folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDdX7DDI8pPN"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIXn7-aF8qdA",
        "outputId": "17b8ae34-314a-4f9e-d2d7-fd6a4920db32"
      },
      "source": [
        "GITHUB_TOKEN = 'fe2e680f071553cddb5f698cc58373a5106380d4'\n",
        "command = f'git clone https://{GITHUB_TOKEN}@github.com/shmulvad/nlp-project.git'\n",
        "!{command}\n",
        "\n",
        "%cd nlp-project/src"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nlp-project'...\n",
            "remote: Enumerating objects: 391, done.\u001b[K\n",
            "remote: Counting objects: 100% (391/391), done.\u001b[K\n",
            "remote: Compressing objects: 100% (276/276), done.\u001b[K\n",
            "remote: Total 391 (delta 168), reused 320 (delta 100), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (391/391), 377.19 KiB | 2.81 MiB/s, done.\n",
            "Resolving deltas: 100% (168/168), done.\n",
            "/content/nlp-project/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36_uvX1B8sJC",
        "outputId": "3a882745-a6d3-4e46-9f8d-84b7d051eb29"
      },
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh > /dev/null | sudo bash\n",
        "!sudo apt-get install git-lfs -qq > /dev/null\n",
        "!git lfs install"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_eb6uDy9ZGs",
        "outputId": "5e8958ea-fc3e-4310-a7f1-aa631e6d69b1"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.19.5)\n",
            "Collecting wordninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/15/abe4af50f4be92b60c25e43c1c64d08453b51e46c32981d80b3aebec0260/wordninja-2.0.0.tar.gz (541kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.6.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (54.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (20.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (8.7.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (0.7.1)\n",
            "Building wheels for collected packages: wordninja\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-cp37-none-any.whl size=541554 sha256=3af979eeaef9e17990149c55981be08776dea2c1980281a15f4eb9d4061ce2fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/46/06/9b6d10ed02c85e93c3bb33ac50e2d368b2586248f192a2e22a\n",
            "Successfully built wordninja\n",
            "Installing collected packages: wordninja\n",
            "Successfully installed wordninja-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImHFIlic-pwd",
        "outputId": "e3e789fd-7dc4-42ca-eae9-47485f488780"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Kj5datp-1LZ",
        "outputId": "06b79acc-5560-4580-8a60-b02859e3ff59"
      },
      "source": [
        "!cp /content/drive/MyDrive/DataFiles/nlp-project-main/src/featurizer_colab.py .\n",
        "!cp //content/drive/MyDrive/DataFiles/nlp-project-main/src/read_data_colab.py .\n",
        "!cp /content/drive/MyDrive/DataFiles/nlp-project-main/src/url_tokenizer.py .\n",
        "!cp /content/drive/MyDrive/DataFiles/nlp-project-main/src/util.py .\n",
        "\n",
        "!pip install wordninja"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wordninja in /usr/local/lib/python3.7/dist-packages (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsZUwUuU-Dbn"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import re\n",
        "import string\n",
        "import pickle\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from url_tokenizer import url_tokenizer\n",
        "from featurizer_colab import UrlFeaturizer, GLOVE, CONCEPTNET, SAMPLE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZuSjsNw-Dbr"
      },
      "source": [
        "### Reading DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6M0BzGq-Dbs"
      },
      "source": [
        "with open('nlp-data/datasets.pkl', 'rb') as f:\n",
        "  datasets = pickle.load(f)\n",
        "\n",
        "dmoz = datasets['dmoz']\n",
        "phishing = datasets['phishing']\n",
        "ilp = datasets['ilp']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrbouGif-Dbs"
      },
      "source": [
        "### Generate Feature Vector\n",
        "\n",
        "This part is only to be used if you want to re-engineer the feature Vector. Since this is an expensive task, I have saved the feature vectors in the initially decided engineering and made them available to be loaded directly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wHFlmldD-Dbt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "60a53855-39c4-4c67-f328-db7d52cbd1df"
      },
      "source": [
        "UF = UrlFeaturizer(GLOVE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading the GloVe word vector file...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8f585d03671b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mUF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUrlFeaturizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGLOVE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/nlp-project/src/featurizer_colab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embedding, sub_domain_max_len, main_domain_max_len, path_max_len, arg_max_len, verbose)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_max_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_max_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__calc_n__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__read_embeddings__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'the'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munknown_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__create_unknown_vec__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/nlp-project/src/featurizer_colab.py\u001b[0m in \u001b[0;36m__read_embeddings__\u001b[0;34m(self, embedding)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__read_conceptnet_embeddings__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGLOVE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__read_glove_embeddings__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSAMPLE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__read_sample_embeddings__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/nlp-project/src/featurizer_colab.py\u001b[0m in \u001b[0;36m__read_glove_embeddings__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m         words_df = pd.read_csv(GLOVE_FILE, sep=\" \", index_col=0,\n\u001b[1;32m    145\u001b[0m                                \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                                header=None, quoting=csv.QUOTE_NONE)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating dictionary index...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb8hyFW6-Dbu"
      },
      "source": [
        "url_regex = re.compile(r'''\n",
        "        (https?):\\/\\/                                   # http s\n",
        "        ([-a-zA-Z0-9@:%._\\+~#=]+\\.[a-zA-Z0-9()]{1,12})  # domains\n",
        "        \\b\n",
        "        ([-a-zA-Z0-9()@:%_\\+;.~#&//=]*)                 # path\n",
        "        \\??\n",
        "        ([-a-zA-Z0-9()@:%_\\+;.~#&//=?]*)                # args\n",
        "    ''', re.DOTALL | re.VERBOSE)\n",
        "\n",
        "def GenerateFeatureVector(df, UF):\n",
        "    df.reset_index(drop = True, inplace = True)\n",
        "    df = pd.get_dummies(data=df, columns=['label'])\n",
        "    label = df.drop(['idx','url'],axis=1).values\n",
        "\n",
        "    url_features = list()\n",
        "    labels = list()\n",
        "\n",
        "    inputs1 = Input(shape=(31, 300))\n",
        "    lstm1 = LSTM(100)(inputs1)\n",
        "    model = Model(inputs=inputs1, outputs=lstm1)\n",
        "\n",
        "    for i in tqdm(range(df.shape[0])):\n",
        "        url = df.iloc[i].url\n",
        "        match = url_regex.match(url.lower())\n",
        "        if match:\n",
        "            features = UF.featurize(url)\n",
        "            temp =   model.predict(features[1].reshape(1,31,300)).reshape(100)\n",
        "            labels.append(label[i])\n",
        "            url_features.append(np.concatenate((temp,features[0])))\n",
        "\n",
        "    url_features = np.array(url_features)\n",
        "    labels = np.array(labels)\n",
        "    return url_features,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHkXCZME-Dbu"
      },
      "source": [
        "Change the df value to the Dataframe you want to generate features and labels for "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nseOPt0K-Dbu"
      },
      "source": [
        "df = phishing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6bF3QcW-Dbv"
      },
      "source": [
        "url_features,labels = GenerateFeatureVector(df,UF)\n",
        "\n",
        "np.save('url_features_phishing.npy', url_features)\n",
        "np.save('labels_phishing.npy', labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdnhxpRs-Dbv"
      },
      "source": [
        "### Load feature Vector\n",
        "\n",
        "Here you can directly load the generated feature vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-4P9QsY-Dbv"
      },
      "source": [
        "#### Load dmoz vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gcVyqDN-Dbw"
      },
      "source": [
        "url_features_dmoz = np.load('/content/drive/MyDrive/DataFiles/nlp-project-main/src/Pickled_Vectors/url_features_dmoz.npy')\n",
        "labels_dmoz = np.load('/content/drive/MyDrive/DataFiles/nlp-project-main/src/Pickled_Vectors/labels_dmoz.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flfO6q8a-Dbw"
      },
      "source": [
        "#### Load phishing vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l72tNo8R-Dbw"
      },
      "source": [
        "url_features_phishing = np.load('/content/drive/MyDrive/DataFiles/nlp-project-main/src/Pickled_Vectors/url_features_phishing.npy')\n",
        "labels_phishing = np.load('/content/drive/MyDrive/DataFiles/nlp-project-main/src/Pickled_Vectors/labels_phishing.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-MIRMQD-Dbx"
      },
      "source": [
        "# Phishing Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-2WxgOL-Dbx"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(url_features_phishing,labels_phishing,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXkzFTw6-Dbx"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bji4DjkN-Dby"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train,Y_train)\n",
        "Y_RF_pred=rf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BILth_Mt-Dby",
        "outputId": "c69e1e07-ec9a-4f15-f229-597b0c734210"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(Y_test,Y_RF_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.46      0.45     11064\n",
            "           1       0.12      0.12      0.12      7073\n",
            "\n",
            "   micro avg       0.33      0.33      0.33     18137\n",
            "   macro avg       0.29      0.29      0.29     18137\n",
            "weighted avg       0.32      0.33      0.32     18137\n",
            " samples avg       0.33      0.33      0.33     18137\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZU-HC5o-Dby"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYD-2r1f-Dbz",
        "outputId": "47951e43-e0a8-4c8d-8046-4213e94c555d"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svc = SVC()\n",
        "svc.fit(X_train,Y_train)\n",
        "Y_SVM_pred=svc.predict(X_test)\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(Y_test,Y_SVM_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.00      0.01      7052\n",
            "           1       0.61      1.00      0.76     11085\n",
            "\n",
            "    accuracy                           0.61     18137\n",
            "   macro avg       0.53      0.50      0.38     18137\n",
            "weighted avg       0.55      0.61      0.47     18137\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8cDJT1--Db0"
      },
      "source": [
        "## Deep Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkgVVGMg-Db0"
      },
      "source": [
        "phishing_features_deep = url_features_phishing.reshape(*url_features_phishing.shape, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3jJNipR-Db1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(phishing_features_deep,labels_phishing,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9TnKV9T-Db1",
        "outputId": "521a3a71-aae2-4f2b-b1cd-90ae4738455e"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Bidirectional, TimeDistributed, Conv1D, ZeroPadding1D, GRU\n",
        "from tensorflow.keras.layers import Lambda, Input, Dropout, Masking, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def cnn_lstm(input_dim, output_dim, dropout=0.2, n_layers=1):\n",
        "    dtype = 'float64'\n",
        "    input_data = Input(name='the_input', shape=input_dim, dtype=dtype)\n",
        "    x = Conv1D(filters=256, kernel_size=10, strides=4, name='conv_1')(input_data)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(dropout, name='dropout_1')(x)\n",
        "        \n",
        "    x = LSTM(128, activation='relu', return_sequences=True,dropout=dropout, name='lstm_1')(x)\n",
        "    x = LSTM(128, activation='relu', return_sequences=False,dropout=dropout, name='lstm_2')(x)\n",
        "\n",
        "    # 1 fully connected layer DNN ReLu with default 20% dropout\n",
        "    x = Dense(units=64, activation='relu', name='fc')(x)\n",
        "    x = Dropout(dropout, name='dropout_2')(x)\n",
        "\n",
        "    # Output layer with softmax\n",
        "    y_pred = Dense(units=output_dim, activation='softmax', name='softmax')(x)\n",
        "    model = Model(inputs=input_data, outputs=y_pred)   \n",
        "    return model\n",
        "\n",
        "input_dim = (107,1)\n",
        "classes = 2\n",
        "K.clear_session()\n",
        "model = cnn_lstm(input_dim, classes)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "the_input (InputLayer)       [(None, 107, 1)]          0         \n",
            "_________________________________________________________________\n",
            "conv_1 (Conv1D)              (None, 25, 256)           2816      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 25, 256)           1024      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 25, 256)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 25, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 25, 128)           197120    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "fc (Dense)                   (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "softmax (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 340,930\n",
            "Trainable params: 340,418\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3sY9BZ4-Db1",
        "outputId": "1e71573a-0a28-4943-f709-07ee6e38eabc"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "sgd = SGD(lr=0.00001, clipnorm=1.0)\n",
        "adam = Adam(lr=1e-4, clipnorm=1.0)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(X_train, Y_train,\n",
        "                    batch_size=128, epochs=10,\n",
        "                    validation_data=(X_test, Y_test),\n",
        "                    callbacks=[TensorBoard(log_dir='logs',\n",
        "                                           histogram_freq=1,\n",
        "                                           update_freq='epoch')])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "567/567 [==============================] - 119s 153ms/step - loss: 0.6431 - accuracy: 0.5997 - val_loss: 0.5912 - val_accuracy: 0.6079\n",
            "Epoch 2/10\n",
            "567/567 [==============================] - 84s 149ms/step - loss: 0.5857 - accuracy: 0.6045 - val_loss: 0.5715 - val_accuracy: 0.6074\n",
            "Epoch 3/10\n",
            "567/567 [==============================] - 84s 149ms/step - loss: 0.5796 - accuracy: 0.6061 - val_loss: 0.5679 - val_accuracy: 0.6118\n",
            "Epoch 4/10\n",
            "567/567 [==============================] - 84s 147ms/step - loss: 0.5754 - accuracy: 0.6069 - val_loss: 0.5663 - val_accuracy: 0.6110\n",
            "Epoch 5/10\n",
            "567/567 [==============================] - 84s 149ms/step - loss: 0.5734 - accuracy: 0.6048 - val_loss: 0.5664 - val_accuracy: 0.6123\n",
            "Epoch 6/10\n",
            "567/567 [==============================] - 84s 149ms/step - loss: 0.5721 - accuracy: 0.6060 - val_loss: 0.5645 - val_accuracy: 0.6079\n",
            "Epoch 7/10\n",
            "567/567 [==============================] - 84s 148ms/step - loss: 0.5718 - accuracy: 0.6056 - val_loss: 0.5632 - val_accuracy: 0.6121\n",
            "Epoch 8/10\n",
            "567/567 [==============================] - 84s 149ms/step - loss: 0.5718 - accuracy: 0.6078 - val_loss: 0.5644 - val_accuracy: 0.6120\n",
            "Epoch 9/10\n",
            "567/567 [==============================] - 84s 148ms/step - loss: 0.5686 - accuracy: 0.6077 - val_loss: 0.5623 - val_accuracy: 0.6120\n",
            "Epoch 10/10\n",
            "567/567 [==============================] - 84s 148ms/step - loss: 0.5704 - accuracy: 0.6054 - val_loss: 0.5621 - val_accuracy: 0.6134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUVws2HT-Db2"
      },
      "source": [
        "# URL Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMPJe1DJ-Db2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(url_features_dmoz,labels_dmoz,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9YnJ1jd-Db2"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73tIC-ur-Db3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train,Y_train)\n",
        "Y_RF_pred=rf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykzs6BGk-Db3"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(Y_test,Y_RF_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEh05wXp-Db3"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XmS3gr6-Db3",
        "outputId": "4ffdb930-4f49-45bb-e906-34ed28552d5b"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svc = SVC()\n",
        "svc.fit(X_train,Y_train)\n",
        "Y_SVM_pred=svc.predict(X_test)\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(Y_test,Y_SVM_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.00      0.01      7052\n",
            "           1       0.61      1.00      0.76     11085\n",
            "\n",
            "    accuracy                           0.61     18137\n",
            "   macro avg       0.53      0.50      0.38     18137\n",
            "weighted avg       0.55      0.61      0.47     18137\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAo4lfXm-Db4"
      },
      "source": [
        "## Deep Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n7wf0Ez-Db4"
      },
      "source": [
        "dmoz_features_deep = url_features_dmoz.reshape(url_features_dmoz.shape[0],url_features_dmoz.shape[1],1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyJ7WfUE-Db4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(dmoz_features_deep,labels_dmoz,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rnn11hKq-Db4"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Bidirectional, TimeDistributed, Conv1D, ZeroPadding1D, GRU\n",
        "from tensorflow.keras.layers import Lambda, Input, Dropout, Masking, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def cnn_lstm(input_dim, output_dim, dropout=0.2, n_layers=1):\n",
        "    dtype = 'float64'\n",
        "    input_data = Input(name='the_input', shape=input_dim, dtype=dtype)\n",
        "#     x = Conv1D(filters=256, kernel_size=10, strides=4, name='conv_1')(input_data)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Activation('relu')(x)\n",
        "#     x = Dropout(dropout, name='dropout_1')(x)\n",
        "        \n",
        "    x = LSTM(128, activation='relu', return_sequences=True,dropout=dropout, name='lstm_1')(input_data)\n",
        "    x = LSTM(128, activation='relu', return_sequences=False,dropout=dropout, name='lstm_2')(x)\n",
        "\n",
        "    # 1 fully connected layer DNN ReLu with default 20% dropout\n",
        "    x = Dense(units=64, activation='relu', name='fc')(x)\n",
        "    x = Dropout(dropout, name='dropout_2')(x)\n",
        "\n",
        "    # Output layer with softmax\n",
        "    y_pred = Dense(units=output_dim, activation='softmax', name='softmax')(x)\n",
        "    model = Model(inputs=input_data, outputs=y_pred)   \n",
        "    return model\n",
        "\n",
        "input_dim = (67,1)\n",
        "classes = 15\n",
        "K.clear_session()\n",
        "model = cnn_lstm(input_dim, classes)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAQM-ms2-Db5"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "sgd = SGD(lr=0.00001, clipnorm=1.0)\n",
        "adam = Adam(lr=1e-4, clipnorm=1.0)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(X_train, Y_train,\n",
        "                    batch_size=128, epochs=10,\n",
        "#                     validation_data=(X_val, Y_val),\n",
        "                    callbacks=[TensorBoard(log_dir='logs',\n",
        "                                           histogram_freq=1,\n",
        "                                           update_freq='epoch')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PNkPFOm-Db5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}