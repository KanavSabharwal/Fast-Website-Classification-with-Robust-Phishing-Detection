{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comparing Embeddings ILP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ6SNYq_tVVC"
      },
      "source": [
        "# Encoding Word Embeddings\n",
        "\n",
        "\n",
        "\n",
        "URLS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg3WVlaMvCLY"
      },
      "source": [
        "# Constants\n",
        "\n",
        "Here, you can alter the dataset, maximum number of samples to use, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2L15ZravBU2"
      },
      "source": [
        "DMOZ, ILP, PHISHING = 'dmoz', 'ilp', 'phishing'\n",
        "\n",
        "DATASET = ILP  # one of the above datsets\n",
        "MAX_NUM_SAMPLES = 1_000_000\n",
        "\n",
        "TEST_PROP = 0.2\n",
        "VAL_PROP = 0.2\n",
        "EPOCHS = 8\n",
        "\n",
        "SEED = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCjmX4zTCkRK"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w_XlxN1IsRJ"
      },
      "source": [
        "You will use the AdamW optimizer from [tensorflow/models](https://github.com/tensorflow/models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V211dJtTvNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89a641f-6578-464a-900d-aeaf21d0a1b9"
      },
      "source": [
        "GITHUB_TOKEN = 'fe2e680f071553cddb5f698cc58373a5106380d4'\n",
        "command = f'git clone --depth 1 https://{GITHUB_TOKEN}@github.com/shmulvad/nlp-project.git'\n",
        "!{command}\n",
        "\n",
        "%cd nlp-project/src"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nlp-project'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 58 (delta 6), reused 33 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (58/58), done.\n",
            "/content/nlp-project/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwaG2TGYN3c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8ae8ea-cc99-4b6c-e016-bc0862f2df9c"
      },
      "source": [
        "!pip install gdown\n",
        "\n",
        "# DMOZ, ILP and original phishing dataset - datasets.pkl\n",
        "!gdown --id 1WV1JSevCnaWY0-mqQMmtOEFSC3Y_Qdg_\n",
        "\n",
        "print('\\n')\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WV1JSevCnaWY0-mqQMmtOEFSC3Y_Qdg_\n",
            "To: /content/nlp-project/src/datasets.pkl\n",
            "99.4MB [00:01, 97.8MB/s]\n",
            "\n",
            "\n",
            " baselines\t\t    demo.ipynb\t       self_trained_embeddings.py\n",
            "'Colab Notebooks'\t    featurizer.py      tests\n",
            " create_fasttext_embed.py   pickle_data.py     url_tokenizer.py\n",
            " data\t\t\t    read_data.py       util.py\n",
            " datasets.pkl\t\t    requirements.txt   word_embed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXdo1h1lUqoO",
        "outputId": "daeec5e9-5bec-49b0-fbce-9eda9ec7a8d3"
      },
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q tensorflow-text\n",
        "!pip install -q tf-models-official\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.4MB 7.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 8.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 24.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 1.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 40.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 11.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 40.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 706kB 34.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 41.0MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.19.5)\n",
            "Collecting wordninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/15/abe4af50f4be92b60c25e43c1c64d08453b51e46c32981d80b3aebec0260/wordninja-2.0.0.tar.gz (541kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.6.4)\n",
            "Collecting gensim==4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/52/f1417772965652d4ca6f901515debcd9d6c5430969e8c02ee7737e6de61c/gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9MB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (54.2.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (8.7.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (1.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (20.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 4)) (0.7.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.1->-r requirements.txt (line 5)) (4.2.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.1->-r requirements.txt (line 5)) (1.4.1)\n",
            "Building wheels for collected packages: wordninja\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-cp37-none-any.whl size=541554 sha256=0f9566f889f900e2d356e3cfe2c1d36fb52b0c86989fbecf621e193a50b2e239\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/46/06/9b6d10ed02c85e93c3bb33ac50e2d368b2586248f192a2e22a\n",
            "Successfully built wordninja\n",
            "Installing collected packages: wordninja, gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.1 wordninja-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ5DQ06ZUupE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48af97a2-2d52-4e61-a571-567d7724c027"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optmizer\n",
        "\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import FastTextKeyedVectors\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set()\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "np.random.seed(SEED)\n",
        "\n",
        "from read_data import read_all_datasets\n",
        "from url_tokenizer import url_tokenizer, flatten_url_data\n",
        "from featurizer import UrlFeaturizer, GLOVE, CONCEPTNET, SAMPLE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "yQBY-p4JuMvm",
        "outputId": "deaf6e85-702c-49bb-b686-dde3b1ded381"
      },
      "source": [
        "with open('datasets.pkl', 'rb') as f:\n",
        "  datasets = pickle.load(f)\n",
        "\n",
        "assert DATASET in datasets, \\\n",
        "  f'You tried to load {DATASET} but only {list(datasets.keys())} available.'\n",
        "\n",
        "dataset = datasets[DATASET]\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>url</th>\n",
              "      <th>label</th>\n",
              "      <th>uni</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6476</th>\n",
              "      <td>6476</td>\n",
              "      <td>http://www.cs.bu.edu/students/grads/dm/Home.html</td>\n",
              "      <td>student</td>\n",
              "      <td>misc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7615</th>\n",
              "      <td>7615</td>\n",
              "      <td>http://www.cs.umn.edu/Research/cpc/</td>\n",
              "      <td>project</td>\n",
              "      <td>misc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4962</th>\n",
              "      <td>4962</td>\n",
              "      <td>http://www.cs.wisc.edu/~dyer/cs540/courses.html</td>\n",
              "      <td>other</td>\n",
              "      <td>wisconsin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5089</th>\n",
              "      <td>5089</td>\n",
              "      <td>http://www.cs.wisc.edu/~greg/cs302/late.html</td>\n",
              "      <td>other</td>\n",
              "      <td>wisconsin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7102</th>\n",
              "      <td>7102</td>\n",
              "      <td>http://www.cs.washington.edu/homes/sds/</td>\n",
              "      <td>student</td>\n",
              "      <td>washington</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2225</th>\n",
              "      <td>2225</td>\n",
              "      <td>http://www.tc.cornell.edu:80/DX/</td>\n",
              "      <td>other</td>\n",
              "      <td>cornell</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8217</th>\n",
              "      <td>8217</td>\n",
              "      <td>http://www.cs.wisc.edu/~pdevries/pdevries.html</td>\n",
              "      <td>staff</td>\n",
              "      <td>wisconsin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6336</th>\n",
              "      <td>6336</td>\n",
              "      <td>http://www.cs.ucr.edu/~sparekh/</td>\n",
              "      <td>student</td>\n",
              "      <td>misc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3904</th>\n",
              "      <td>3904</td>\n",
              "      <td>http://www.cs.washington.edu/homes/marclang/re...</td>\n",
              "      <td>other</td>\n",
              "      <td>washington</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1028</th>\n",
              "      <td>1028</td>\n",
              "      <td>http://www.cs.washington.edu/homes/etzioni/</td>\n",
              "      <td>faculty</td>\n",
              "      <td>washington</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8230 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       idx  ...         uni\n",
              "6476  6476  ...        misc\n",
              "7615  7615  ...        misc\n",
              "4962  4962  ...   wisconsin\n",
              "5089  5089  ...   wisconsin\n",
              "7102  7102  ...  washington\n",
              "...    ...  ...         ...\n",
              "2225  2225  ...     cornell\n",
              "8217  8217  ...   wisconsin\n",
              "6336  6336  ...        misc\n",
              "3904  3904  ...  washington\n",
              "1028  1028  ...  washington\n",
              "\n",
              "[8230 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb58ujMqHrfj",
        "outputId": "eaf322d7-ad04-4166-a7b4-58fa69d1269a"
      },
      "source": [
        "# Download respectively model and ngrams\n",
        "if DATASET == DMOZ:\n",
        "  !gdown --id 1V8_EWQTF_JhgEVbXIvAiHViTHyIO9CBS\n",
        "  !gdown --id 15EirC5KybMrG33RvYUXIfnBM2GWSBdh5\n",
        "elif DATASET == ILP:\n",
        "  !gdown --id 1_QafULaXKmq0H0fwY3dkR-WOKwsyUkHS\n",
        "  !gdown --id 1kn4EbllSLdAX-8Ca5db6_BewLugCAWp5\n",
        "else:  # Phishing\n",
        "  !gdown --id 1JEsUG4eGqbIItBH468xph-AsS3iQ5Fif\n",
        "  !gdown --id 10p0XPRsuTsXblQNNlUricEmd8if0B2P6\n",
        "\n",
        "fast_text_embedding = FastTextKeyedVectors.load(f'embed-{DATASET}.model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_QafULaXKmq0H0fwY3dkR-WOKwsyUkHS\n",
            "To: /content/nlp-project/src/embed-ilp.model\n",
            "100% 528k/528k [00:00<00:00, 3.22MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kn4EbllSLdAX-8Ca5db6_BewLugCAWp5\n",
            "To: /content/nlp-project/src/embed-ilp.model.vectors_ngrams.npy\n",
            "800MB [00:17, 45.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmTt5L8TU6kM"
      },
      "source": [
        "# Testing different Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dz2rapfZvtx"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.layers import LSTM,Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uJKWlq-iE8g"
      },
      "source": [
        "## Different approaches to Encode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI2tXenm2zlL"
      },
      "source": [
        "#### Method 1 - FLatten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l440yMEOd3bs"
      },
      "source": [
        "Here we simply flatten the word embeddings for each URL and append the handpicked featrues\n",
        "\n",
        "Thus, the (31,100) embeddings are flattened to (3100,) vector.\n",
        "Further appending the handpicked features makes the vector shape: (3120,)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8XrF0MhTjqS"
      },
      "source": [
        "url_regex = re.compile(r'''\n",
        "        (https?):\\/\\/                                   # http s\n",
        "        ([-a-zA-Z0-9@:%._\\+~#=]+\\.[a-zA-Z0-9()]{1,12})  # domains\n",
        "        \\b\n",
        "        ([-a-zA-Z0-9()@:%_\\+;.~#&//=]*)                 # path\n",
        "        \\??\n",
        "        ([-a-zA-Z0-9()@:%_\\+;.~#&//=?]*)                # args\n",
        "    ''', re.DOTALL | re.VERBOSE)\n",
        "\n",
        "def GenerateFeatureVector_Flatten(df, UF):\n",
        "    df.reset_index(drop = True, inplace = True)\n",
        "    df = pd.get_dummies(data=df, columns=['label'])\n",
        "    label = df.drop(['idx','url','uni'],axis=1).values\n",
        "\n",
        "    url_features = list()\n",
        "    labels = list()\n",
        "\n",
        "    for i in tqdm(range(df.shape[0])):\n",
        "        url = df.iloc[i].url\n",
        "        match = url_regex.match(url.lower())\n",
        "        if match:\n",
        "            features = UF.featurize(url)\n",
        "            temp =   features[1].flatten()\n",
        "            labels.append(label[i])\n",
        "            url_features.append(np.concatenate((temp,features[0])))\n",
        "\n",
        "    url_features = np.array(url_features)\n",
        "    labels = np.array(labels)\n",
        "    return url_features,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeIG2ZCz8Spa"
      },
      "source": [
        "#### Enocoder 2 --> Piecewise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVKkyccslPZ4"
      },
      "source": [
        "def valid_url(url):\n",
        "  match = url_regex.match(url.lower())\n",
        "  if match:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "url_regex = re.compile(r'''\n",
        "        (https?):\\/\\/                                   # http s\n",
        "        ([-a-zA-Z0-9@:%._\\+~#=]+\\.[a-zA-Z0-9()]{1,12})  # domains\n",
        "        \\b\n",
        "        ([-a-zA-Z0-9()@:%_\\+;.~#&//=]*)                 # path\n",
        "        \\??\n",
        "        ([-a-zA-Z0-9()@:%_\\+;.~#&//=?]*)                # args\n",
        "    ''', re.DOTALL | re.VERBOSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdDIVHod8NxV"
      },
      "source": [
        "def GenerateFeatureVector_Enc2(dataset, UF, glove = False):\n",
        "    df = dataset.copy()\n",
        "\n",
        "    df.reset_index(drop = True, inplace = True)\n",
        "    df['valid'] = df.url.apply(lambda x:valid_url(x))\n",
        "    df = df[df['valid']==True]\n",
        "    df.drop(['valid'],axis=1,inplace=True)\n",
        "    df = pd.get_dummies(data=df, columns=['label'])\n",
        "    labels = df.drop(['idx','url','uni'],axis=1).values\n",
        "\n",
        "    raw_features = feat.featurize(np.array(df.url))\n",
        "\n",
        "    embeddings = list()\n",
        "    hand_picked = list()\n",
        "    for feature in raw_features:\n",
        "      embeddings.append(feature[1])\n",
        "      hand_picked.append(feature[0])\n",
        "\n",
        "    embeddings = np.array(embeddings)\n",
        "    hand_picked = np.array(hand_picked)\n",
        "\n",
        "    sub_domain = embeddings[:,:5,:]\n",
        "    main_domain = embeddings[:,5:10,:]\n",
        "    domain_end_vec = embeddings[:,10,:]\n",
        "    path = embeddings[:,11:21,:]\n",
        "    arg = embeddings[:,21:,:]\n",
        "\n",
        "    if glove:\n",
        "      inputs1 = Input(shape=(5, 300))\n",
        "      lstm1 = LSTM(100)(inputs1)\n",
        "      model1 = Model(inputs=inputs1, outputs=lstm1)\n",
        "\n",
        "      inputs2 = Input(shape=(10, 300))\n",
        "      lstm2 = LSTM(100)(inputs2)\n",
        "      model2 = Model(inputs=inputs2, outputs=lstm2)\n",
        "    else:\n",
        "      inputs1 = Input(shape=(5, 100))\n",
        "      lstm1 = LSTM(100)(inputs1)\n",
        "      model1 = Model(inputs=inputs1, outputs=lstm1)\n",
        "\n",
        "      inputs2 = Input(shape=(10, 100))\n",
        "      lstm2 = LSTM(100)(inputs2)\n",
        "      model2 = Model(inputs=inputs2, outputs=lstm2)\n",
        "\n",
        "    sub_domain_encoded = model1.predict(sub_domain)\n",
        "    main_domain_encoded = model1.predict(main_domain)\n",
        "    path_encoded = model2.predict(path)\n",
        "    arg_encoded = model2.predict(arg)\n",
        "\n",
        "    features = np.concatenate([sub_domain_encoded,main_domain_encoded,domain_end_vec,path_encoded,arg_encoded],axis=1)\n",
        "\n",
        "    return features,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lPPXTOFU_TU"
      },
      "source": [
        "##Fast Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_RKHQ4Yp_D0"
      },
      "source": [
        "sampled_dataset = dataset.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz_e3TpwilUU",
        "outputId": "b4017ba7-7fe9-407b-a605-0d0d9d0ed56f"
      },
      "source": [
        "feat = UrlFeaturizer(embedding=fast_text_embedding)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating the average vector of all the word vectors...\n",
            "Created FastText UrlFeaturizer in 0.0 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBnXSpjNW6qd"
      },
      "source": [
        "### Testing Features by Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myKYmTVVW9zT"
      },
      "source": [
        "url_features,labels = GenerateFeatureVector_Enc2(sampled_dataset,feat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6A0FsslXFjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db501d68-9894-498e-8cbe-04ac1f951260"
      },
      "source": [
        "X_train,X_test,Y_train,Y_test=train_test_split(url_features,labels,test_size=0.2)\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train,Y_train)\n",
        "Y_RF_pred=rf.predict(X_test)\n",
        "print(classification_report(Y_test,Y_RF_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.72      0.79       186\n",
            "           1       0.92      0.59      0.72        39\n",
            "           2       0.82      0.54      0.65       213\n",
            "           3       0.88      0.81      0.84       737\n",
            "           4       0.82      0.36      0.50        99\n",
            "           5       0.00      0.00      0.00        39\n",
            "           6       0.76      0.65      0.70       333\n",
            "\n",
            "   micro avg       0.85      0.68      0.75      1646\n",
            "   macro avg       0.73      0.52      0.60      1646\n",
            "weighted avg       0.82      0.68      0.74      1646\n",
            " samples avg       0.68      0.68      0.68      1646\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlxVxJHvY5Qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a6c403-bfe5-4ba1-83fe-30fe2e785bac"
      },
      "source": [
        "print(f1_score(Y_test,Y_RF_pred,average = 'weighted'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7399557043784316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaIKCJrzVo8q"
      },
      "source": [
        "## GLoVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jsNAZciVvWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0cccb29-fa33-4303-b5b7-7d895937c026"
      },
      "source": [
        "feat = UrlFeaturizer(embedding='GloVe')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading the glove-wiki-gigaword-300 word vector file...\n",
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n",
            "Creating the average vector of all the word vectors...\n",
            "Created GloVe UrlFeaturizer in 280.0 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruhfg-BPXShe"
      },
      "source": [
        "### Testing featrues by Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqKMLsn6XSqo"
      },
      "source": [
        "url_features,labels = GenerateFeatureVector_Enc2(sampled_dataset,feat,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVgSgqrGXSux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e73680-eb88-4ce0-a3c4-837a664bba7c"
      },
      "source": [
        "X_train,X_test,Y_train,Y_test=train_test_split(url_features,labels,test_size=0.2)\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train,Y_train)\n",
        "Y_RF_pred=rf.predict(X_test)\n",
        "print(classification_report(Y_test,Y_RF_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.55      0.69       190\n",
            "           1       1.00      0.57      0.72        37\n",
            "           2       0.90      0.35      0.51       232\n",
            "           3       0.87      0.84      0.85       747\n",
            "           4       0.92      0.25      0.39        97\n",
            "           5       0.00      0.00      0.00        27\n",
            "           6       0.78      0.62      0.69       316\n",
            "\n",
            "   micro avg       0.86      0.64      0.73      1646\n",
            "   macro avg       0.77      0.45      0.55      1646\n",
            "weighted avg       0.86      0.64      0.71      1646\n",
            " samples avg       0.64      0.64      0.64      1646\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4xDK4QIXSzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86cd8849-e690-491e-d7fb-c36453282d80"
      },
      "source": [
        "print(f1_score(Y_test,Y_RF_pred,average = 'weighted'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7104947291378031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlPO9HL4Vzlq"
      },
      "source": [
        "## Word2Vec Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nalMhalmV1YT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4002ffdb-2ad7-4906-8609-3e11d75c82d6"
      },
      "source": [
        "feat = UrlFeaturizer(embedding='Word2Vec')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading the word2vec-google-news-300 word vector file...\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Creating the average vector of all the word vectors...\n",
            "Created Word2Vec UrlFeaturizer in 622.0 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYgaY-cNXT49"
      },
      "source": [
        "### Testing featrues by Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je8rTcWZVL-Q",
        "outputId": "8a615fb5-e342-423e-95c7-02d619bda038"
      },
      "source": [
        "import gc\n",
        "dataset = None\n",
        "datasets = None\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4450"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K72MFUQhXUBE"
      },
      "source": [
        "url_features,labels = GenerateFeatureVector_Enc2(sampled_dataset,feat, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTvLeGIMXUEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be26061b-9700-4c83-b48e-c09c3c998846"
      },
      "source": [
        "X_train,X_test,Y_train,Y_test=train_test_split(url_features,labels,test_size=0.2)\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train,Y_train)\n",
        "Y_RF_pred=rf.predict(X_test)\n",
        "print(classification_report(Y_test,Y_RF_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.71      0.80       174\n",
            "           1       0.96      0.59      0.73        44\n",
            "           2       0.81      0.27      0.41       234\n",
            "           3       0.87      0.79      0.83       766\n",
            "           4       0.79      0.20      0.32        94\n",
            "           5       0.00      0.00      0.00        20\n",
            "           6       0.72      0.55      0.62       314\n",
            "\n",
            "   micro avg       0.84      0.61      0.71      1646\n",
            "   macro avg       0.72      0.44      0.53      1646\n",
            "weighted avg       0.82      0.61      0.68      1646\n",
            " samples avg       0.61      0.61      0.61      1646\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxRZ77YCXUIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3d5362-052a-4f37-ab80-7eb132ef090d"
      },
      "source": [
        "print(f1_score(Y_test,Y_RF_pred,average = 'weighted'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6839293796123764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDGcLAMWcgyE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}